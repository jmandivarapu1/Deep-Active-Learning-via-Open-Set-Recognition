{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRN(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexibly sized Wide Residual Network (WRN). Extended to the variational setting.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, num_classes, num_colors, args):\n",
    "        super(WRN, self).__init__()\n",
    "        \n",
    "        self.encoder = make_layers([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],batch_norm=True)\n",
    "        \n",
    "        # self.encoder = nn.Sequential(OrderedDict([\n",
    "        #     ('encoder_conv1', nn.Conv2d(num_colors, self.nChannels[0], kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        #     ('encoder_block1', WRNNetworkBlock(self.num_block_layers, self.nChannels[0], self.nChannels[1],\n",
    "        #                                        WRNBasicBlock, batchnorm=self.batch_norm, dropout=self.dropout)),\n",
    "        #     ('encoder_block2', WRNNetworkBlock(self.num_block_layers, self.nChannels[1], self.nChannels[2],\n",
    "        #                                        WRNBasicBlock, batchnorm=self.batch_norm, stride=2,\n",
    "        #                                        dropout=self.dropout)),\n",
    "        #     ('encoder_block3', WRNNetworkBlock(self.num_block_layers, self.nChannels[2], self.nChannels[3],\n",
    "        #                                        WRNBasicBlock, batchnorm=self.batch_norm, stride=2,\n",
    "        #                                        dropout=self.dropout)),\n",
    "        #     ('encoder_bn1', nn.BatchNorm2d(self.nChannels[3], eps=self.batch_norm)),\n",
    "        #     ('encoder_act1', nn.ReLU(inplace=True))\n",
    "        # ]))\n",
    "\n",
    "        self.enc_channels, self.enc_spatial_dim_x, self.enc_spatial_dim_y = get_feat_size(self.encoder, self.patch_size,self.num_colors)\n",
    "        if self.variational:\n",
    "            #print(\"True it's in the args variational\")\n",
    "            self.latent_mu = nn.Linear(self.enc_spatial_dim_x * self.enc_spatial_dim_x * self.enc_channels,self.latent_dim, bias=False)\n",
    "            self.latent_std = nn.Linear(self.enc_spatial_dim_x * self.enc_spatial_dim_y * self.enc_channels,self.latent_dim, bias=False)\n",
    "            self.latent_feat_out = self.latent_dim\n",
    "        else:\n",
    "            self.latent_feat_out = self.enc_spatial_dim_x * self.enc_spatial_dim_x * self.enc_channels\n",
    "            self.latent_dim = self.latent_feat_out\n",
    "            #print(self.latent_dim)\n",
    "\n",
    "        if self.joint:\n",
    "            #print(\"True came to joint training\")\n",
    "            self.classifier = nn.Sequential(nn.Linear(self.latent_feat_out, num_classes, bias=False))\n",
    "\n",
    "            if self.variational:\n",
    "                #print(\"True came to variational\",self.latent_feat_out,self.enc_spatial_dim_x * self.enc_spatial_dim_y *\n",
    "                                               # self.enc_channels)\n",
    "                self.latent_decoder = nn.Linear(self.latent_feat_out, self.enc_spatial_dim_x * self.enc_spatial_dim_y *\n",
    "                                                self.enc_channels, bias=False)\n",
    "\n",
    "            self.decoder =  nn.Sequential(\n",
    "            # nn.Linear(1,self.enc_spatial_dim_x * self.enc_spatial_dim_y *\n",
    "            #                                     self.enc_channels),                           # B, 1024*8*8\n",
    "            # View((-1, 1024, 4, 4)),                               # B, 1024,  8,  8\n",
    "            nn.ConvTranspose2d(512, 512, 3, 1, 1, bias=False),   # B,  512, 16, 16\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 3, 1, 1, bias=False),    # B,  256, 32, 32\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 3, 1, 1, bias=False),    # B,  128, 64, 64\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 3, 1),                       # B,   nc, 64, 64\n",
    "        )\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            self.classifier = nn.Sequential(nn.Linear(self.latent_feat_out, num_classes, bias=False))\n",
    "\n",
    "        # self._initialize_weights()\n",
    "\n",
    "    def encode(self, x):\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        if self.variational:\n",
    "            x = x.view(x.size(0), -1)\n",
    "            z_mean = self.latent_mu(x)\n",
    "            z_std = self.latent_std(x)\n",
    "            return z_mean, z_std\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def reparameterize(self, mu, std):\n",
    "        eps = std.data.new(std.size()).normal_()\n",
    "        return eps.mul(std).add(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        if self.variational:\n",
    "            z = self.latent_decoder(z)\n",
    "            z = z.view(z.size(0), self.enc_channels, self.enc_spatial_dim_x, self.enc_spatial_dim_y)\n",
    "            #print(\"shape of z is\",z.shape)\n",
    "        x = self.decoder(z)\n",
    "        return x\n",
    "\n",
    "    def generate(self):\n",
    "        z = torch.randn(self.batch_size, self.latent_dim).to(self.device)\n",
    "        x = self.decode(z)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.variational:\n",
    "            z_mean, z_std = self.encode(x)\n",
    "            if self.joint:\n",
    "                output_samples = torch.zeros(self.num_samples, x.size(0), self.num_colors, self.patch_size,\n",
    "                                             self.patch_size).to(self.device)\n",
    "                classification_samples = torch.zeros(self.num_samples, x.size(0), self.num_classes).to(self.device)\n",
    "            else:\n",
    "                output_samples = torch.zeros(self.num_samples, x.size(0), self.num_classes).to(self.device)\n",
    "            for i in range(self.num_samples):\n",
    "                z = self.reparameterize(z_mean, z_std)\n",
    "                if self.joint:\n",
    "                    output_samples[i] = self.decode(z)\n",
    "                    classification_samples[i] = self.classifier(z)\n",
    "                else:\n",
    "                    output_samples[i] = self.classifier(z)\n",
    "            if self.joint:\n",
    "                return classification_samples, output_samples, z_mean, z_std\n",
    "            else:\n",
    "                return output_samples, z_mean, z_std\n",
    "        else:\n",
    "            x = self.encode(x)\n",
    "            if self.joint:\n",
    "                recon = self.decode(x)\n",
    "                classification = self.classifier(x.view(x.size(0), -1))\n",
    "                return classification, recon\n",
    "            else:\n",
    "                output = self.classifier(x.view(x.size(0), -1))\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('preds', 'rb') as fp:preds = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].shape#.view(128,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "preds=torch.argmax(preds[0], dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('labels', 'rb') as fp:labels = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
       "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
       "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
       "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
       "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7,\n",
       "        8, 3, 1, 2, 8, 0, 8, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(labels, preds, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataloader =  data.DataLoader(\n",
    "                datasets.Caltech256('/mnt/iscsi/data/Jay/datasets/', download=True),\n",
    "            batch_size=128, drop_last=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30720"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "240*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets import *\n",
    "def imagenet_transformer():\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                              std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader =  data.DataLoader(datasets.Caltech256('/mnt/iscsi/data/Jay/datasets/caltech256/', download=True,transform=imagenet_transformer()),\n",
    "#             batch_size=128, drop_last=False)\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data\n",
    "class Caltech2556(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.caltech256 =  datasets.ImageFolder(root=path,transform=transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, numpy.float64):\n",
    "            index = index.astype(numpy.int64)\n",
    "        data, target = self.caltech256[index]\n",
    "\n",
    "        return data, target, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caltech256)\n",
    "    \n",
    "train_dataset = Caltech2556('/mnt/iscsi/data/Jay/datasets/caltech256/caltech256/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001.ak47',\n",
       " '002.american-flag',\n",
       " '003.backpack',\n",
       " '004.baseball-bat',\n",
       " '005.baseball-glove',\n",
       " '006.basketball-hoop',\n",
       " '007.bat',\n",
       " '008.bathtub',\n",
       " '009.bear',\n",
       " '010.beer-mug',\n",
       " '011.billiards',\n",
       " '012.binoculars',\n",
       " '013.birdbath',\n",
       " '014.blimp',\n",
       " '015.bonsai-101',\n",
       " '016.boom-box',\n",
       " '017.bowling-ball',\n",
       " '018.bowling-pin',\n",
       " '019.boxing-glove',\n",
       " '020.brain-101',\n",
       " '021.breadmaker',\n",
       " '022.buddha-101',\n",
       " '023.bulldozer',\n",
       " '024.butterfly',\n",
       " '025.cactus',\n",
       " '026.cake',\n",
       " '027.calculator',\n",
       " '028.camel',\n",
       " '029.cannon',\n",
       " '030.canoe',\n",
       " '031.car-tire',\n",
       " '032.cartman',\n",
       " '033.cd',\n",
       " '034.centipede',\n",
       " '035.cereal-box',\n",
       " '036.chandelier-101',\n",
       " '037.chess-board',\n",
       " '038.chimp',\n",
       " '039.chopsticks',\n",
       " '040.cockroach',\n",
       " '041.coffee-mug',\n",
       " '042.coffin',\n",
       " '043.coin',\n",
       " '044.comet',\n",
       " '045.computer-keyboard',\n",
       " '046.computer-monitor',\n",
       " '047.computer-mouse',\n",
       " '048.conch',\n",
       " '049.cormorant',\n",
       " '050.covered-wagon',\n",
       " '051.cowboy-hat',\n",
       " '052.crab-101',\n",
       " '053.desk-globe',\n",
       " '054.diamond-ring',\n",
       " '055.dice',\n",
       " '056.dog',\n",
       " '057.dolphin-101',\n",
       " '058.doorknob',\n",
       " '059.drinking-straw',\n",
       " '060.duck',\n",
       " '061.dumb-bell',\n",
       " '062.eiffel-tower',\n",
       " '063.electric-guitar-101',\n",
       " '064.elephant-101',\n",
       " '065.elk',\n",
       " '066.ewer-101',\n",
       " '067.eyeglasses',\n",
       " '068.fern',\n",
       " '069.fighter-jet',\n",
       " '070.fire-extinguisher',\n",
       " '071.fire-hydrant',\n",
       " '072.fire-truck',\n",
       " '073.fireworks',\n",
       " '074.flashlight',\n",
       " '075.floppy-disk',\n",
       " '076.football-helmet',\n",
       " '077.french-horn',\n",
       " '078.fried-egg',\n",
       " '079.frisbee',\n",
       " '080.frog',\n",
       " '081.frying-pan',\n",
       " '082.galaxy',\n",
       " '083.gas-pump',\n",
       " '084.giraffe',\n",
       " '085.goat',\n",
       " '086.golden-gate-bridge',\n",
       " '087.goldfish',\n",
       " '088.golf-ball',\n",
       " '089.goose',\n",
       " '090.gorilla',\n",
       " '091.grand-piano-101',\n",
       " '092.grapes',\n",
       " '093.grasshopper',\n",
       " '094.guitar-pick',\n",
       " '095.hamburger',\n",
       " '096.hammock',\n",
       " '097.harmonica',\n",
       " '098.harp',\n",
       " '099.harpsichord',\n",
       " '100.hawksbill-101',\n",
       " '101.head-phones',\n",
       " '102.helicopter-101',\n",
       " '103.hibiscus',\n",
       " '104.homer-simpson',\n",
       " '105.horse',\n",
       " '106.horseshoe-crab',\n",
       " '107.hot-air-balloon',\n",
       " '108.hot-dog',\n",
       " '109.hot-tub',\n",
       " '110.hourglass',\n",
       " '111.house-fly',\n",
       " '112.human-skeleton',\n",
       " '113.hummingbird',\n",
       " '114.ibis-101',\n",
       " '115.ice-cream-cone',\n",
       " '116.iguana',\n",
       " '117.ipod',\n",
       " '118.iris',\n",
       " '119.jesus-christ',\n",
       " '120.joy-stick',\n",
       " '121.kangaroo-101',\n",
       " '122.kayak',\n",
       " '123.ketch-101',\n",
       " '124.killer-whale',\n",
       " '125.knife',\n",
       " '126.ladder',\n",
       " '127.laptop-101',\n",
       " '128.lathe',\n",
       " '129.leopards-101',\n",
       " '130.license-plate',\n",
       " '131.lightbulb',\n",
       " '132.light-house',\n",
       " '133.lightning',\n",
       " '134.llama-101',\n",
       " '135.mailbox',\n",
       " '136.mandolin',\n",
       " '137.mars',\n",
       " '138.mattress',\n",
       " '139.megaphone',\n",
       " '140.menorah-101',\n",
       " '141.microscope',\n",
       " '142.microwave',\n",
       " '143.minaret',\n",
       " '144.minotaur',\n",
       " '145.motorbikes-101',\n",
       " '146.mountain-bike',\n",
       " '147.mushroom',\n",
       " '148.mussels',\n",
       " '149.necktie',\n",
       " '150.octopus',\n",
       " '151.ostrich',\n",
       " '152.owl',\n",
       " '153.palm-pilot',\n",
       " '154.palm-tree',\n",
       " '155.paperclip',\n",
       " '156.paper-shredder',\n",
       " '157.pci-card',\n",
       " '158.penguin',\n",
       " '159.people',\n",
       " '160.pez-dispenser',\n",
       " '161.photocopier',\n",
       " '162.picnic-table',\n",
       " '163.playing-card',\n",
       " '164.porcupine',\n",
       " '165.pram',\n",
       " '166.praying-mantis',\n",
       " '167.pyramid',\n",
       " '168.raccoon',\n",
       " '169.radio-telescope',\n",
       " '170.rainbow',\n",
       " '171.refrigerator',\n",
       " '172.revolver-101',\n",
       " '173.rifle',\n",
       " '174.rotary-phone',\n",
       " '175.roulette-wheel',\n",
       " '176.saddle',\n",
       " '177.saturn',\n",
       " '178.school-bus',\n",
       " '179.scorpion-101',\n",
       " '180.screwdriver',\n",
       " '181.segway',\n",
       " '182.self-propelled-lawn-mower',\n",
       " '183.sextant',\n",
       " '184.sheet-music',\n",
       " '185.skateboard',\n",
       " '186.skunk',\n",
       " '187.skyscraper',\n",
       " '188.smokestack',\n",
       " '189.snail',\n",
       " '190.snake',\n",
       " '191.sneaker',\n",
       " '192.snowmobile',\n",
       " '193.soccer-ball',\n",
       " '194.socks',\n",
       " '195.soda-can',\n",
       " '196.spaghetti',\n",
       " '197.speed-boat',\n",
       " '198.spider',\n",
       " '199.spoon',\n",
       " '200.stained-glass',\n",
       " '201.starfish-101',\n",
       " '202.steering-wheel',\n",
       " '203.stirrups',\n",
       " '204.sunflower-101',\n",
       " '205.superman',\n",
       " '206.sushi',\n",
       " '207.swan',\n",
       " '208.swiss-army-knife',\n",
       " '209.sword',\n",
       " '210.syringe',\n",
       " '211.tambourine',\n",
       " '212.teapot',\n",
       " '213.teddy-bear',\n",
       " '214.teepee',\n",
       " '215.telephone-box',\n",
       " '216.tennis-ball',\n",
       " '217.tennis-court',\n",
       " '218.tennis-racket',\n",
       " '219.theodolite',\n",
       " '220.toaster',\n",
       " '221.tomato',\n",
       " '222.tombstone',\n",
       " '223.top-hat',\n",
       " '224.touring-bike',\n",
       " '225.tower-pisa',\n",
       " '226.traffic-light',\n",
       " '227.treadmill',\n",
       " '228.triceratops',\n",
       " '229.tricycle',\n",
       " '230.trilobite-101',\n",
       " '231.tripod',\n",
       " '232.t-shirt',\n",
       " '233.tuning-fork',\n",
       " '234.tweezer',\n",
       " '235.umbrella-101',\n",
       " '236.unicorn',\n",
       " '237.vcr',\n",
       " '238.video-projector',\n",
       " '239.washing-machine',\n",
       " '240.watch-101',\n",
       " '241.waterfall',\n",
       " '242.watermelon',\n",
       " '243.welding-mask',\n",
       " '244.wheelbarrow',\n",
       " '245.windmill',\n",
       " '246.wine-bottle',\n",
       " '247.xylophone',\n",
       " '248.yarmulke',\n",
       " '249.yo-yo',\n",
       " '250.zebra',\n",
       " '251.airplanes-101',\n",
       " '252.car-side-101',\n",
       " '253.faces-easy-101',\n",
       " '254.greyhound',\n",
       " '255.tennis-shoes',\n",
       " '256.toad']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.caltech256.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29780"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([3, 224, 224]) 0 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-77d4d27ee95f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#if i%10==0:print(i,images.shape,target,_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "for i, (images, target,_) in enumerate(train_dataset):\n",
    "    #if i%10==0:print(i,images.shape,target,_)\n",
    "    print(i,images.shape,target,_)\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = 128120\n",
    "num_images = 30607\n",
    "budget = 64060\n",
    "initial_budget = 3000\n",
    "num_classes = 257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='/mnt/iscsi/data/Jay/datasets/cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-fc3a08268159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_dataloader = data.DataLoader(\n\u001b[0;32m----> 2\u001b[0;31m                 \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m             batch_size=128, drop_last=False)\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_path' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataloader = data.DataLoader(\n",
    "                datasets.CIFAR10(data_path, download=True, train=False),\n",
    "            batch_size=128, drop_last=False)\n",
    "train_dataset = CIFAR10(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "args = EasyDict()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_val = 2700\n",
    "args.num_images = 27607\n",
    "args.budget = 1530\n",
    "args.initial_budget = 3060\n",
    "args.num_classes = 256\n",
    "args.test=3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'budget': 1530,\n",
       " 'initial_budget': 3060,\n",
       " 'num_classes': 256,\n",
       " 'num_images': 27607,\n",
       " 'num_val': 2700,\n",
       " 'test': 3000}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "all_indices = set(np.arange(args.num_images))\n",
    "val_indices = random.sample(all_indices, args.num_val)\n",
    "all_indices = np.setdiff1d(list(all_indices), val_indices)\n",
    "initial_indices = random.sample(list(all_indices), args.initial_budget)\n",
    "sampler = data.sampler.SubsetRandomSampler(initial_indices)\n",
    "val_sampler = data.sampler.SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=torch.Tensor(2,64,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.2678e-41, 0.0000e+00, 3.2680e-41,  ..., 0.0000e+00,\n",
       "          3.2856e-41, 0.0000e+00],\n",
       "         [3.2858e-41, 0.0000e+00, 3.2859e-41,  ..., 0.0000e+00,\n",
       "          3.3036e-41, 0.0000e+00],\n",
       "         [3.3037e-41, 0.0000e+00, 3.3038e-41,  ..., 0.0000e+00,\n",
       "          3.3215e-41, 0.0000e+00],\n",
       "         ...,\n",
       "         [4.9326e-42, 0.0000e+00, 4.9340e-42,  ..., 0.0000e+00,\n",
       "          5.1105e-42, 0.0000e+00],\n",
       "         [5.1119e-42, 0.0000e+00, 5.1133e-42,  ..., 0.0000e+00,\n",
       "          5.2899e-42, 0.0000e+00],\n",
       "         [5.2913e-42, 0.0000e+00, 5.2927e-42,  ..., 0.0000e+00,\n",
       "          5.4693e-42, 0.0000e+00]],\n",
       "\n",
       "        [[5.4707e-42, 0.0000e+00, 5.4721e-42,  ..., 0.0000e+00,\n",
       "          5.6486e-42, 0.0000e+00],\n",
       "         [5.6500e-42, 0.0000e+00, 5.6514e-42,  ..., 0.0000e+00,\n",
       "          5.8280e-42, 0.0000e+00],\n",
       "         [5.8294e-42, 0.0000e+00, 5.8308e-42,  ..., 0.0000e+00,\n",
       "          6.0074e-42, 0.0000e+00],\n",
       "         ...,\n",
       "         [1.6412e-41, 0.0000e+00, 1.6413e-41,  ..., 0.0000e+00,\n",
       "          1.6590e-41, 0.0000e+00],\n",
       "         [1.6591e-41, 0.0000e+00, 1.6593e-41,  ..., 0.0000e+00,\n",
       "          1.6769e-41, 0.0000e+00],\n",
       "         [1.6771e-41, 0.0000e+00, 1.6772e-41,  ..., 0.0000e+00,\n",
       "          1.6949e-41, 0.0000e+00]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.2678e-41, 0.0000e+00, 3.2680e-41,  ..., 0.0000e+00,\n",
       "          3.2856e-41, 0.0000e+00],\n",
       "         [3.2858e-41, 0.0000e+00, 3.2859e-41,  ..., 0.0000e+00,\n",
       "          3.3036e-41, 0.0000e+00],\n",
       "         [3.3037e-41, 0.0000e+00, 3.3038e-41,  ..., 0.0000e+00,\n",
       "          3.3215e-41, 0.0000e+00],\n",
       "         ...,\n",
       "         [1.6412e-41, 0.0000e+00, 1.6413e-41,  ..., 0.0000e+00,\n",
       "          1.6590e-41, 0.0000e+00],\n",
       "         [1.6591e-41, 0.0000e+00, 1.6593e-41,  ..., 0.0000e+00,\n",
       "          1.6769e-41, 0.0000e+00],\n",
       "         [1.6771e-41, 0.0000e+00, 1.6772e-41,  ..., 0.0000e+00,\n",
       "          1.6949e-41, 0.0000e+00]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=torch.Tensor(1,128,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=torch.cat((a[0], a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=a.view_as(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('output_samples_', 'rb') as fp:output_samples_ = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_samples_[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('target', 'rb') as fp:target = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 82, 256, 229,  47, 190,  68, 239,  54,  18,  12, 112, 176,   1, 125,\n",
       "          4, 199, 141, 202, 224, 256, 250,   3, 171, 223,  50, 147,  80, 180,\n",
       "        190,  43, 109,   9, 252, 104,  28,  69,  73, 207,  57,  69, 202, 116,\n",
       "         57, 256, 229,  10, 211,  41,  44, 215,  91, 106,  43, 152, 223, 236,\n",
       "        189, 240, 148,  93, 118,  21, 232, 241, 242, 231, 163, 142, 252,  24,\n",
       "         89, 139, 144, 157,  97, 185, 142, 159, 209, 156, 190, 124,  91, 113,\n",
       "        104, 239, 153,  63,  86,  97,  37,  33,  33,   1,  60, 151,  94, 254,\n",
       "         29, 245,  82, 195, 256,  52,  38,  76, 253, 171, 144, 117, 213, 109,\n",
       "        126, 213, 162, 155, 215, 178, 104,  86, 243,  27,  22,  76, 123, 180,\n",
       "         49, 243], device='cuda:0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
