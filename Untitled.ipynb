{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRN(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexibly sized Wide Residual Network (WRN). Extended to the variational setting.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, num_classes, num_colors, args):\n",
    "        super(WRN, self).__init__()\n",
    "        \n",
    "        self.encoder = make_layers([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],batch_norm=True)\n",
    "        \n",
    "        # self.encoder = nn.Sequential(OrderedDict([\n",
    "        #     ('encoder_conv1', nn.Conv2d(num_colors, self.nChannels[0], kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        #     ('encoder_block1', WRNNetworkBlock(self.num_block_layers, self.nChannels[0], self.nChannels[1],\n",
    "        #                                        WRNBasicBlock, batchnorm=self.batch_norm, dropout=self.dropout)),\n",
    "        #     ('encoder_block2', WRNNetworkBlock(self.num_block_layers, self.nChannels[1], self.nChannels[2],\n",
    "        #                                        WRNBasicBlock, batchnorm=self.batch_norm, stride=2,\n",
    "        #                                        dropout=self.dropout)),\n",
    "        #     ('encoder_block3', WRNNetworkBlock(self.num_block_layers, self.nChannels[2], self.nChannels[3],\n",
    "        #                                        WRNBasicBlock, batchnorm=self.batch_norm, stride=2,\n",
    "        #                                        dropout=self.dropout)),\n",
    "        #     ('encoder_bn1', nn.BatchNorm2d(self.nChannels[3], eps=self.batch_norm)),\n",
    "        #     ('encoder_act1', nn.ReLU(inplace=True))\n",
    "        # ]))\n",
    "\n",
    "        self.enc_channels, self.enc_spatial_dim_x, self.enc_spatial_dim_y = get_feat_size(self.encoder, self.patch_size,self.num_colors)\n",
    "        if self.variational:\n",
    "            #print(\"True it's in the args variational\")\n",
    "            self.latent_mu = nn.Linear(self.enc_spatial_dim_x * self.enc_spatial_dim_x * self.enc_channels,self.latent_dim, bias=False)\n",
    "            self.latent_std = nn.Linear(self.enc_spatial_dim_x * self.enc_spatial_dim_y * self.enc_channels,self.latent_dim, bias=False)\n",
    "            self.latent_feat_out = self.latent_dim\n",
    "        else:\n",
    "            self.latent_feat_out = self.enc_spatial_dim_x * self.enc_spatial_dim_x * self.enc_channels\n",
    "            self.latent_dim = self.latent_feat_out\n",
    "            #print(self.latent_dim)\n",
    "\n",
    "        if self.joint:\n",
    "            #print(\"True came to joint training\")\n",
    "            self.classifier = nn.Sequential(nn.Linear(self.latent_feat_out, num_classes, bias=False))\n",
    "\n",
    "            if self.variational:\n",
    "                #print(\"True came to variational\",self.latent_feat_out,self.enc_spatial_dim_x * self.enc_spatial_dim_y *\n",
    "                                               # self.enc_channels)\n",
    "                self.latent_decoder = nn.Linear(self.latent_feat_out, self.enc_spatial_dim_x * self.enc_spatial_dim_y *\n",
    "                                                self.enc_channels, bias=False)\n",
    "\n",
    "            self.decoder =  nn.Sequential(\n",
    "            # nn.Linear(1,self.enc_spatial_dim_x * self.enc_spatial_dim_y *\n",
    "            #                                     self.enc_channels),                           # B, 1024*8*8\n",
    "            # View((-1, 1024, 4, 4)),                               # B, 1024,  8,  8\n",
    "            nn.ConvTranspose2d(512, 512, 3, 1, 1, bias=False),   # B,  512, 16, 16\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 3, 1, 1, bias=False),    # B,  256, 32, 32\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 3, 1, 1, bias=False),    # B,  128, 64, 64\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 3, 1),                       # B,   nc, 64, 64\n",
    "        )\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            self.classifier = nn.Sequential(nn.Linear(self.latent_feat_out, num_classes, bias=False))\n",
    "\n",
    "        # self._initialize_weights()\n",
    "\n",
    "    def encode(self, x):\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        if self.variational:\n",
    "            x = x.view(x.size(0), -1)\n",
    "            z_mean = self.latent_mu(x)\n",
    "            z_std = self.latent_std(x)\n",
    "            return z_mean, z_std\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def reparameterize(self, mu, std):\n",
    "        eps = std.data.new(std.size()).normal_()\n",
    "        return eps.mul(std).add(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        if self.variational:\n",
    "            z = self.latent_decoder(z)\n",
    "            z = z.view(z.size(0), self.enc_channels, self.enc_spatial_dim_x, self.enc_spatial_dim_y)\n",
    "            #print(\"shape of z is\",z.shape)\n",
    "        x = self.decoder(z)\n",
    "        return x\n",
    "\n",
    "    def generate(self):\n",
    "        z = torch.randn(self.batch_size, self.latent_dim).to(self.device)\n",
    "        x = self.decode(z)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.variational:\n",
    "            z_mean, z_std = self.encode(x)\n",
    "            if self.joint:\n",
    "                output_samples = torch.zeros(self.num_samples, x.size(0), self.num_colors, self.patch_size,\n",
    "                                             self.patch_size).to(self.device)\n",
    "                classification_samples = torch.zeros(self.num_samples, x.size(0), self.num_classes).to(self.device)\n",
    "            else:\n",
    "                output_samples = torch.zeros(self.num_samples, x.size(0), self.num_classes).to(self.device)\n",
    "            for i in range(self.num_samples):\n",
    "                z = self.reparameterize(z_mean, z_std)\n",
    "                if self.joint:\n",
    "                    output_samples[i] = self.decode(z)\n",
    "                    classification_samples[i] = self.classifier(z)\n",
    "                else:\n",
    "                    output_samples[i] = self.classifier(z)\n",
    "            if self.joint:\n",
    "                return classification_samples, output_samples, z_mean, z_std\n",
    "            else:\n",
    "                return output_samples, z_mean, z_std\n",
    "        else:\n",
    "            x = self.encode(x)\n",
    "            if self.joint:\n",
    "                recon = self.decode(x)\n",
    "                classification = self.classifier(x.view(x.size(0), -1))\n",
    "                return classification, recon\n",
    "            else:\n",
    "                output = self.classifier(x.view(x.size(0), -1))\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('preds', 'rb') as fp:preds = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].shape#.view(128,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "preds=torch.argmax(preds[0], dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('labels', 'rb') as fp:labels = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
       "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
       "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
       "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
       "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7,\n",
       "        8, 3, 1, 2, 8, 0, 8, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(labels, preds, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataloader =  data.DataLoader(\n",
    "                datasets.Caltech256('/mnt/iscsi/data/Jay/datasets/', download=True),\n",
    "            batch_size=128, drop_last=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30720"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "240*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataloader =  data.DataLoader(\n",
    "                datasets.Caltech256('/mnt/iscsi/data/Jay/datasets/caltech256/', download=True),\n",
    "            batch_size=128, drop_last=False)\n",
    "train_dataset = Caltech256('/mnt/iscsi/data/Jay/datasets/caltech256/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30607"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = 128120\n",
    "num_images = 30607\n",
    "budget = 64060\n",
    "initial_budget = 3000\n",
    "num_classes = 257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='/mnt/iscsi/data/Jay/datasets/cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = data.DataLoader(\n",
    "                datasets.CIFAR10(data_path, download=True, train=False),\n",
    "            batch_size=128, drop_last=False)\n",
    "train_dataset = CIFAR10(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
